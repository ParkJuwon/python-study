{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순차적인 데이터를 처리하는 RNN (Recurrent neural network)\n",
    "- 영어 두 문장이 있다\n",
    "    - I live to eat.\n",
    "    - I eat to live.\n",
    "- 두 문장 같은 단어로 이뤄 졌지만, live와 eat의 위치가 바뀌면서 뜻이 달라짐\n",
    "- 단어의 특징만 잡아내는 일반적인 신경망 구조로는 이 변화를 인식하기 어려움\n",
    "- 데이터의 순서가 주는 정보까지 인지해내는 새로운 형태의 신경망을 배움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN 개요\n",
    "- 앞에서 배운 신경망 모델들은 시간에 대한 개념이 없다.\n",
    "- 하지만 현실에서 접하는 거의 모든 경험은 순차적이다.\n",
    "- RNN은 순차적 데이터, 시계열 데이터 의 정보를 받아 전체 내용을 학습한다\n",
    "- RNN은 정해지지 않은 길이의 배열을 읽고 설명하는 신경망\n",
    "    - 시계열 데이터인 주가 정보를 입력받았다고 가정 순서대로 데이터를 훑으며 정보를 파악하고 등락률이 얼마나 될지 예측할수 있다\n",
    "- RNN의 출력은 순차적 데이터의 흐름을 모두 내포\n",
    "- RNN은 시계열 데이터의 정보를 하나씩 입력 받을때마다 지금까지 입력받은 벡터들을 종합해 은닉 벡터를 만들어 낸다\n",
    "    - 입력1,2,3...k 까지 입력 받을 때 마다 모두 압축한 은닉벡터 생성 \n",
    "    - 마지막 은닉벡터$_k$는 배열 속 모든 벡터들의 내용을 압축한 벡터라고 할 수 있다\n",
    "<img src=\"./7-1_RNN구조.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "- RNN 계열 신경망들은 대표적인 시계열 데이터인 텍스트와 자연어를 처리하고 학습하는데 주로 사용\n",
    "    - LSTM(long short term memory), GRU(gated recurrent unit) 등 응용 RNN 이 개발되어 언어 모델링(language modeling), 텍스트 감정 분석(text seniment analysis), 기계 번역(machine translation) 등의 분야에서 이용\n",
    "- RNN을 응용한 신경망 형태의 예\n",
    "    - 일대일\n",
    "        - 일대일은 일반적으로 보아온 신경망이나 CNN과 같다\n",
    "    <img src=\"./7_2_1_일대일.png\" width=\"20%\" height=\"20%\">\n",
    "    - 일대다\n",
    "        - 일대다는 이미지를 보고 이미지 안의 상황을 글로 설명하는 등의 문제\n",
    "    <img src=\"./7_2_2_일대다.png\" width=\"70%\" height=\"70%\">\n",
    "    - 다대일\n",
    "        - 다대일은 감정 분석 같이 순차적인 데이터를 보고 값 하나를 내는 경우\n",
    "    <img src=\"./7_2_3_다대일.png\" width=\"70%\" height=\"70%\">\n",
    "    - 다대다\n",
    "        - 다대다는 챗봇과 기계 번역 같이 순차적인 데이터를 보고 순차적인 데이터를 출력\n",
    "    <img src=\"./7_2_4_다대다.png\" width=\"70%\" height=\"70%\">\n",
    "    - 다대다\n",
    "        - 비디오 분류 같이 매 프레임을 레이블링 할 때 사용하는 구조\n",
    "    <img src=\"./7_2_5_다대다.png\" width=\"70%\" height=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영화 리뷰 감정 분석\n",
    "- 데이터의 순서 정보를 학습한다는 점에서 RNN 은 CIFAR-10 같은 정적 데이터 보다는 동영상, 자연어, 주가 등 동적인 데이터를 이용할 때 성능이 극대화\n",
    "- 가장 기본적인 자연어 처리 작업이라고 할 수 있는 텍스트 감정 분석\n",
    "- 텍스트 형태의 데이터셋인 IMDB는 영화리뷰 5만건으로 이루어졌다\n",
    "- 다수의 영어 문장으로 구성, 긍정적 리뷰 2, 부정적 1로 레이블링\n",
    "- RNN 을 이용한 영화 리뷰 감정 분석\n",
    "    <img src=\"./7_3_RNN_영화감정분석.png\" width=\"70%\" height=\"70%\">\n",
    "- 토큰나이징\n",
    "    - 언어의 최소 단위인 토큰으로 나누는 것\n",
    "- 워드 임베딩\n",
    "    - 언어의 최소 단위인 토큰을 벡터형태로 변환 하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어의 전처리 과정은 개념적으로 어렵진 않지만 코딩량이 많고 버그를 만들기 쉬움\n",
    "# 다행이도 토치텍스트의 전처리 도구들과 파이토치의 nn.Embedding 같은 기능을 이용하면 된다\n",
    "\n",
    "# 라이브러리 import\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 정의\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "# EPOCHS = 40\n",
    "EPOCHS = 5\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 데이터셋 로딩하고 텐서로 변환\n",
    "# 텍스트 형태 영화 리뷰들과 그에 해당하는 레이블을 텐서로 바꿔줄 때 필요한 설정 정함\n",
    "# sequential 파라미터를 이용해 데이터셋이 순차적인 테이터셋인지 명시\n",
    "# 레이블 값은 단순히 클래스를 나타내는 숫자이므로 순차적인 데이터가 아님\n",
    "# batch_first 파라미터로 신경망에 입력되는 텐서의 첫 번째 차원값이 batch_size가 되도록 정해줌\n",
    "# lower 변수를 이용해 텍스트 데이터 속 모든 영문 알파벳이 소문자가 되도록 처리\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    " \n",
    "# dataset 객체의 splits() 함수를 이용해 모델에 입력되는 데이터셋을 만듬\n",
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# 워드 임베딩에 필요한 단어 사전을 만듬\n",
    "# min_freq 는 학습 데이터에서 최소 5번 이상 등장한 단어만 사전에 담겠다는 의미\n",
    "# 그 미만으로 출현하는 단어는 'unk' (unknown) 토큰으로 대체\n",
    "TEXT.build_vocab(trainset, min_freq=5)\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 데이터셋에선 따로 검증셋이 존재하지 않기에 학습셋을 쪼개서 사용\n",
    "# batch 단위로 쪼개서 학습을 진행\n",
    "trainset, valset = trainset.split(split_ratio=0.8)\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "                                (trainset, valset, testset),\n",
    "                                batch_size=BATCH_SIZE, shuffle=True, repeat=False)\n",
    "\n",
    "# 사전 속 단어들의 개수와 레이블의 수를 정해주는 변수를 만듬\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습셋]: 6554 [검증셋]: 1638 [테스트셋]: 25000 [단어수]: 46159 [클래스]: 2\n"
     ]
    }
   ],
   "source": [
    "print('[학습셋]: %d [검증셋]: %d [테스트셋]: %d [단어수]: %d [클래스]: %d' \n",
    "     % (len(trainset), len(valset), len(testset), vocab_size, n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다층(multilayer) 형태의 RNN\n",
    "<img src=\"./7_4_다층형태.png\" width=\"70%\" height=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 모델 구현\n",
    "class BasicGRU(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p = 0.2):\n",
    "        super(BasicGRU, self).__init__()\n",
    "        print('Building Basic GRU model...')\n",
    "        \n",
    "        # 가장 먼저 정의하는 변수는 은닉 벡터들의 '층' 이라고 할수 있는 n_layers 이다\n",
    "        # 아주 복잡한 모델이 아닌이상 n_layers는 2이하로 정의\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # nn.Embedding() 함수는 두개를 입력 받음\n",
    "        # 첫 번째 파라미터(n_vocab)는 전체 데이터셋의 모든 단어를 사전형태로 나타내었을 때 그 사전에 등재된 단어 수\n",
    "        # 두 번째 파라미터(embed_dim)는 임베딩된 단어 텐서가 지나는 차원 값\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        \n",
    "        # 은닉 벡터의 차원값과 드롭아웃을 정의\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # RNN 모델 정의\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim, \n",
    "                          num_layers=self.n_layers, batch_first=True)\n",
    "        \n",
    "        # RNN이 아닌 GRU 사용이유\n",
    "        # 사람도 긴 문장을 읽다보면 앞부분 맥락이 기억나지 않을때가 있듯 \n",
    "        # 딥러닝 모델도 문장 뒷 부분에 다다를수록 정보가 소실됨\n",
    "        # 기본적인 RNN은 입력이 길어지면 학습 도중 기울기가 너무 작아지거나 커져서 \n",
    "        # 앞부분에 정보를 정확이 못담을수 있음\n",
    "        # 경사도 폭발, 소실 가능\n",
    "        # 결함을 보완하는 더 발전된 형태의 GRU 사용\n",
    "        # GRU는 시계열 데이터 속 벡터 사이의 정보 전달량을 조절\n",
    "        # 기울기를 적정하게 유지, 문장 앞부분의 정보가 끝까지 도달할 수 있도록 도와줌\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 모델에 입력되는 x는 한 배치속에 있는 모든 영화평\n",
    "        # 영화평을 embed() 함수로 워드임베딩 시키면 백터의 배열, 즉 시계열 데이터로 변환\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # 보통의 신경망이라면 이제 바로 신경망 모듈의 forward() 함수를 호출해도 되지만\n",
    "        # RNN 계열 신경망은 입력 데이터 외에도 은닉벡터를 정의해 x와 함께 임력해줘야 함\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 은닉 벡터는 보통 모든 특성값이 0인 벡터로 설정\n",
    "        \n",
    "        # 은닉벡터 h_0과 함께 self.gru() 함수에 입력하면 은닉 벡터들이 시계열 배열 형태로 변환, 3d 텐서\n",
    "        # self.gru() 함수가 반환한 텐서를 [:,-1,:]로 인덱싱 하면 \n",
    "        # 배치내 모든 시계열 은닉 벡터들의 마지막 토큰들을 내포한 (batch_size, 1, hidden_dim) 모양의 텐서 추출\n",
    "        # h_t가 곧 영화 리뷰 배열들을 압축한 은닉 벡터\n",
    "        x, _ = self.gru(x, h_0)\n",
    "        h_t = x[:,-1,:]\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)\n",
    "        return logit\n",
    "        \n",
    "        \n",
    "    def _init_state(self, batch_size=1):\n",
    "        # parameters() 함수는 신경망 모듈 의 가중치 정보들을 반복자 형태로 반환\n",
    "        # next(self.parameters()).data는 nn.GRU 모듈의 첫번째 가중치 텐서를 추출\n",
    "        # 그후 new() 함수를 호출해 모델의 가중치와 같은 모양인 (n_layers, batch_size, hidden_dim) 모양을 \n",
    "        # 갖춘 텐서로 변환후 zero_() 함수를 호출해 텐서 내 모든 값을 0으로 초기화\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수와 평가 함수를 구현\n",
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        \n",
    "        # label은 1 또는 2의 값 -> 0,1 로 바꾸자\n",
    "        y.data.sub_(1) # 모든 값에서 1씩 빼는 함수\n",
    "        \n",
    "        # 매번 기울기를 새로 계산 하므로 기울기를 0으로\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)\n",
    "        \n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1)\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, reduction='sum') # 오차의 합을 구함\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum() # 모델이 맞친 수\n",
    "    \n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Basic GRU model...\n",
      "[이폭: 1] 검증 오차: 0.69 | 검증 정확도:51.00\n",
      "[이폭: 2] 검증 오차: 0.69 | 검증 정확도:50.00\n",
      "[이폭: 3] 검증 오차: 0.70 | 검증 정확도:49.00\n",
      "[이폭: 4] 검증 오차: 0.70 | 검증 정확도:49.00\n",
      "[이폭: 5] 검증 오차: 0.71 | 검증 정확도:50.00\n"
     ]
    }
   ],
   "source": [
    "# 학습 전 모델 객체를 정의\n",
    "# 은닉 벡터 차원값으론 256, 임베딩 토큰의 차원값은 128로 임의 설정\n",
    "# 최적화 알고리즘 선택, 무슨 최적화 알고리즘을 사용할지 모를 때는 Adam을 사용하라는 말이 있음\n",
    "# 최적화 알고리즘 비교 : https://sacko.tistory.com/42\n",
    "\n",
    "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "# 학습\n",
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "    \n",
    "    print('[이폭: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f' % (e, val_loss, val_accuracy))\n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적화 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir('snapshot'):\n",
    "            os.makedirs('snapshot')\n",
    "        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋으로 모델 성능 시험\n",
    "model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iter)\n",
    "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
