{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순차적인 데이터를 처리하는 RNN (Recurrent neural network)\n",
    "- 영어 두 문장이 있다\n",
    "    - I live to eat.\n",
    "    - I eat to live.\n",
    "- 두 문장 같은 단어로 이뤄 졌지만, live와 eat의 위치가 바뀌면서 뜻이 달라짐\n",
    "- 단어의 특징만 잡아내는 일반적인 신경망 구조로는 이 변화를 인식하기 어려움\n",
    "- 데이터의 순서가 주는 정보까지 인지해내는 새로운 형태의 신경망을 배움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN 개요\n",
    "- 앞에서 배운 신경망 모델들은 시간에 대한 개념이 없다.\n",
    "- 하지만 현실에서 접하는 거의 모든 경험은 순차적이다.\n",
    "- RNN은 순차적 데이터, 시계열 데이터 의 정보를 받아 전체 내용을 학습한다\n",
    "- RNN은 정해지지 않은 길이의 배열을 읽고 설명하는 신경망\n",
    "    - 시계열 데이터인 주가 정보를 입력받았다고 가정 순서대로 데이터를 훑으며 정보를 파악하고 등락률이 얼마나 될지 예측할수 있다\n",
    "- RNN의 출력은 순차적 데이터의 흐름을 모두 내포\n",
    "- RNN은 시계열 데이터의 정보를 하나씩 입력 받을때마다 지금까지 입력받은 벡터들을 종합해 은닉 벡터를 만들어 낸다\n",
    "    - 입력1,2,3...k 까지 입력 받을 때 마다 모두 압축한 은닉벡터 생성 \n",
    "    - 마지막 은닉벡터$_k$는 배열 속 모든 벡터들의 내용을 압축한 벡터라고 할 수 있다\n",
    "<img src=\"./7-1_RNN구조.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "- RNN 계열 신경망들은 대표적인 시계열 데이터인 텍스트와 자연어를 처리하고 학습하는데 주로 사용\n",
    "    - LSTM(long short term memory), GRU(gated recurrent unit) 등 응용 RNN 이 개발되어 언어 모델링(language modeling), 텍스트 감정 분석(text seniment analysis), 기계 번역(machine translation) 등의 분야에서 이용\n",
    "- RNN을 응용한 신경망 형태의 예\n",
    "    - 일대일\n",
    "        - 일대일은 일반적으로 보아온 신경망이나 CNN과 같다\n",
    "    <img src=\"./7_2_1_일대일.png\" width=\"20%\" height=\"20%\">\n",
    "    - 일대다\n",
    "        - 일대다는 이미지를 보고 이미지 안의 상황을 글로 설명하는 등의 문제\n",
    "    <img src=\"./7_2_2_일대다.png\" width=\"70%\" height=\"70%\">\n",
    "    - 다대일\n",
    "        - 다대일은 감정 분석 같이 순차적인 데이터를 보고 값 하나를 내는 경우\n",
    "    <img src=\"./7_2_3_다대일.png\" width=\"70%\" height=\"70%\">\n",
    "    - 다대다\n",
    "        - 다대다는 챗봇과 기계 번역 같이 순차적인 데이터를 보고 순차적인 데이터를 출력\n",
    "    <img src=\"./7_2_4_다대다.png\" width=\"70%\" height=\"70%\">\n",
    "    - 다대다\n",
    "        - 비디오 분류 같이 매 프레임을 레이블링 할 때 사용하는 구조\n",
    "    <img src=\"./7_2_5_다대다.png\" width=\"70%\" height=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영화 리뷰 감정 분석\n",
    "- 데이터의 순서 정보를 학습한다는 점에서 RNN 은 CIFAR-10 같은 정적 데이터 보다는 동영상, 자연어, 주가 등 동적인 데이터를 이용할 때 성능이 극대화\n",
    "- 가장 기본적인 자연어 처리 작업이라고 할 수 있는 텍스트 감정 분석\n",
    "- 텍스트 형태의 데이터셋인 IMDB는 영화리뷰 5만건으로 이루어졌다\n",
    "- 다수의 영어 문장으로 구성, 긍정적 리뷰 2, 부정적 1로 레이블링\n",
    "- RNN 을 이용한 영화 리뷰 감정 분석\n",
    "    <img src=\"./7_3_RNN_영화감정분석.png\" width=\"70%\" height=\"70%\">\n",
    "- 토큰나이징\n",
    "    - 언어의 최소 단위인 토큰으로 나누는 것\n",
    "- 워드 임베딩\n",
    "    - 언어의 최소 단위인 토큰을 벡터형태로 변환 하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어의 전처리 과정은 개념적으로 어렵진 않지만 코딩량이 많고 버그를 만들기 쉬움\n",
    "# 다행이도 토치텍스트의 전처리 도구들과 파이토치의 nn.Embedding 같은 기능을 이용하면 된다\n",
    "\n",
    "# 라이브러리 import\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 정의\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 40\n",
    "# EPOCHS = 5\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 데이터셋 로딩하고 텐서로 변환\n",
    "# 텍스트 형태 영화 리뷰들과 그에 해당하는 레이블을 텐서로 바꿔줄 때 필요한 설정 정함\n",
    "# sequential 파라미터를 이용해 데이터셋이 순차적인 테이터셋인지 명시\n",
    "# 레이블 값은 단순히 클래스를 나타내는 숫자이므로 순차적인 데이터가 아님\n",
    "# batch_first 파라미터로 신경망에 입력되는 텐서의 첫 번째 차원값이 batch_size가 되도록 정해줌\n",
    "# lower 변수를 이용해 텍스트 데이터 속 모든 영문 알파벳이 소문자가 되도록 처리\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    " \n",
    "# dataset 객체의 splits() 함수를 이용해 모델에 입력되는 데이터셋을 만듬\n",
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# 워드 임베딩에 필요한 단어 사전을 만듬\n",
    "# min_freq 는 학습 데이터에서 최소 5번 이상 등장한 단어만 사전에 담겠다는 의미\n",
    "# 그 미만으로 출현하는 단어는 'unk' (unknown) 토큰으로 대체\n",
    "TEXT.build_vocab(trainset, min_freq=5)\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 데이터셋에선 따로 검증셋이 존재하지 않기에 학습셋을 쪼개서 사용\n",
    "# batch 단위로 쪼개서 학습을 진행\n",
    "trainset, valset = trainset.split(split_ratio=0.8)\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "                                (trainset, valset, testset),\n",
    "                                batch_size=BATCH_SIZE, shuffle=True, repeat=False)\n",
    "\n",
    "# 사전 속 단어들의 개수와 레이블의 수를 정해주는 변수를 만듬\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습셋]: 20000 [검증셋]: 5000 [테스트셋]: 25000 [단어수]: 46159 [클래스]: 2\n"
     ]
    }
   ],
   "source": [
    "print('[학습셋]: %d [검증셋]: %d [테스트셋]: %d [단어수]: %d [클래스]: %d' \n",
    "     % (len(trainset), len(valset), len(testset), vocab_size, n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다층(multilayer) 형태의 RNN\n",
    "<img src=\"./7_4_다층형태.png\" width=\"70%\" height=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 모델 구현\n",
    "class BasicGRU(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p = 0.2):\n",
    "        super(BasicGRU, self).__init__()\n",
    "        print('Building Basic GRU model...')\n",
    "        \n",
    "        # 가장 먼저 정의하는 변수는 은닉 벡터들의 '층' 이라고 할수 있는 n_layers 이다\n",
    "        # 아주 복잡한 모델이 아닌이상 n_layers는 2이하로 정의\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # nn.Embedding() 함수는 두개를 입력 받음\n",
    "        # 첫 번째 파라미터(n_vocab)는 전체 데이터셋의 모든 단어를 사전형태로 나타내었을 때 그 사전에 등재된 단어 수\n",
    "        # 두 번째 파라미터(embed_dim)는 임베딩된 단어 텐서가 지나는 차원 값\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        \n",
    "        # 은닉 벡터의 차원값과 드롭아웃을 정의\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # RNN 모델 정의\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim, \n",
    "                          num_layers=self.n_layers, batch_first=True)\n",
    "        \n",
    "        # RNN이 아닌 GRU 사용이유\n",
    "        # 사람도 긴 문장을 읽다보면 앞부분 맥락이 기억나지 않을때가 있듯 \n",
    "        # 딥러닝 모델도 문장 뒷 부분에 다다를수록 정보가 소실됨\n",
    "        # 기본적인 RNN은 입력이 길어지면 학습 도중 기울기가 너무 작아지거나 커져서 \n",
    "        # 앞부분에 정보를 정확이 못담을수 있음\n",
    "        # 경사도 폭발, 소실 가능\n",
    "        # 결함을 보완하는 더 발전된 형태의 GRU 사용\n",
    "        # GRU는 시계열 데이터 속 벡터 사이의 정보 전달량을 조절\n",
    "        # 기울기를 적정하게 유지, 문장 앞부분의 정보가 끝까지 도달할 수 있도록 도와줌\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 모델에 입력되는 x는 한 배치속에 있는 모든 영화평\n",
    "        # 영화평을 embed() 함수로 워드임베딩 시키면 백터의 배열, 즉 시계열 데이터로 변환\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # 보통의 신경망이라면 이제 바로 신경망 모듈의 forward() 함수를 호출해도 되지만\n",
    "        # RNN 계열 신경망은 입력 데이터 외에도 은닉벡터를 정의해 x와 함께 임력해줘야 함\n",
    "        h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 은닉 벡터는 보통 모든 특성값이 0인 벡터로 설정\n",
    "        \n",
    "        # 은닉벡터 h_0과 함께 self.gru() 함수에 입력하면 은닉 벡터들이 시계열 배열 형태로 변환, 3d 텐서\n",
    "        # self.gru() 함수가 반환한 텐서를 [:,-1,:]로 인덱싱 하면 \n",
    "        # 배치내 모든 시계열 은닉 벡터들의 마지막 토큰들을 내포한 (batch_size, 1, hidden_dim) 모양의 텐서 추출\n",
    "        # h_t가 곧 영화 리뷰 배열들을 압축한 은닉 벡터\n",
    "        x, _ = self.gru(x, h_0)\n",
    "        h_t = x[:,-1,:]\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)\n",
    "        return logit\n",
    "        \n",
    "        \n",
    "    def _init_state(self, batch_size=1):\n",
    "        # parameters() 함수는 신경망 모듈 의 가중치 정보들을 반복자 형태로 반환\n",
    "        # next(self.parameters()).data는 nn.GRU 모듈의 첫번째 가중치 텐서를 추출\n",
    "        # 그후 new() 함수를 호출해 모델의 가중치와 같은 모양인 (n_layers, batch_size, hidden_dim) 모양을 \n",
    "        # 갖춘 텐서로 변환후 zero_() 함수를 호출해 텐서 내 모든 값을 0으로 초기화\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수와 평가 함수를 구현\n",
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        \n",
    "        # label은 1 또는 2의 값 -> 0,1 로 바꾸자\n",
    "        y.data.sub_(1) # 모든 값에서 1씩 빼는 함수\n",
    "        \n",
    "        # 매번 기울기를 새로 계산 하므로 기울기를 0으로\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)\n",
    "        \n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1)\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, reduction='sum') # 오차의 합을 구함\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum() # 모델이 맞친 수\n",
    "    \n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Basic GRU model...\n",
      "[이폭: 1] 검증 오차: 0.70 | 검증 정확도:50.00\n",
      "[이폭: 2] 검증 오차: 0.70 | 검증 정확도:50.00\n",
      "[이폭: 3] 검증 오차: 0.61 | 검증 정확도:67.00\n",
      "[이폭: 4] 검증 오차: 0.34 | 검증 정확도:85.00\n",
      "[이폭: 5] 검증 오차: 0.35 | 검증 정확도:85.00\n",
      "[이폭: 6] 검증 오차: 0.37 | 검증 정확도:86.00\n",
      "[이폭: 7] 검증 오차: 0.40 | 검증 정확도:85.00\n",
      "[이폭: 8] 검증 오차: 0.42 | 검증 정확도:86.00\n",
      "[이폭: 9] 검증 오차: 0.46 | 검증 정확도:87.00\n",
      "[이폭: 10] 검증 오차: 0.49 | 검증 정확도:86.00\n",
      "[이폭: 11] 검증 오차: 0.50 | 검증 정확도:87.00\n",
      "[이폭: 12] 검증 오차: 0.48 | 검증 정확도:86.00\n",
      "[이폭: 13] 검증 오차: 0.57 | 검증 정확도:86.00\n",
      "[이폭: 14] 검증 오차: 0.58 | 검증 정확도:86.00\n",
      "[이폭: 15] 검증 오차: 0.56 | 검증 정확도:86.00\n",
      "[이폭: 16] 검증 오차: 0.60 | 검증 정확도:86.00\n",
      "[이폭: 17] 검증 오차: 0.58 | 검증 정확도:87.00\n",
      "[이폭: 18] 검증 오차: 0.60 | 검증 정확도:87.00\n",
      "[이폭: 19] 검증 오차: 0.58 | 검증 정확도:87.00\n",
      "[이폭: 20] 검증 오차: 0.66 | 검증 정확도:86.00\n",
      "[이폭: 21] 검증 오차: 0.69 | 검증 정확도:86.00\n",
      "[이폭: 22] 검증 오차: 0.62 | 검증 정확도:86.00\n",
      "[이폭: 23] 검증 오차: 0.63 | 검증 정확도:87.00\n",
      "[이폭: 24] 검증 오차: 0.62 | 검증 정확도:87.00\n",
      "[이폭: 25] 검증 오차: 0.63 | 검증 정확도:87.00\n",
      "[이폭: 26] 검증 오차: 0.69 | 검증 정확도:87.00\n",
      "[이폭: 27] 검증 오차: 0.75 | 검증 정확도:87.00\n",
      "[이폭: 28] 검증 오차: 0.76 | 검증 정확도:87.00\n",
      "[이폭: 29] 검증 오차: 0.77 | 검증 정확도:87.00\n",
      "[이폭: 30] 검증 오차: 0.79 | 검증 정확도:87.00\n",
      "[이폭: 31] 검증 오차: 0.80 | 검증 정확도:87.00\n",
      "[이폭: 32] 검증 오차: 0.82 | 검증 정확도:87.00\n",
      "[이폭: 33] 검증 오차: 0.83 | 검증 정확도:87.00\n",
      "[이폭: 34] 검증 오차: 0.85 | 검증 정확도:87.00\n",
      "[이폭: 35] 검증 오차: 0.86 | 검증 정확도:87.00\n",
      "[이폭: 36] 검증 오차: 0.88 | 검증 정확도:87.00\n",
      "[이폭: 37] 검증 오차: 0.89 | 검증 정확도:87.00\n",
      "[이폭: 38] 검증 오차: 0.90 | 검증 정확도:87.00\n",
      "[이폭: 39] 검증 오차: 0.92 | 검증 정확도:87.00\n",
      "[이폭: 40] 검증 오차: 0.93 | 검증 정확도:87.00\n"
     ]
    }
   ],
   "source": [
    "# 학습 전 모델 객체를 정의\n",
    "# 은닉 벡터 차원값으론 256, 임베딩 토큰의 차원값은 128로 임의 설정\n",
    "# 최적화 알고리즘 선택, 무슨 최적화 알고리즘을 사용할지 모를 때는 Adam을 사용하라는 말이 있음\n",
    "# 최적화 알고리즘 비교 : https://sacko.tistory.com/42\n",
    "\n",
    "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "# 학습\n",
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "    \n",
    "    print('[이폭: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f' % (e, val_loss, val_accuracy))\n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적화 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir('snapshot'):\n",
    "            os.makedirs('snapshot')\n",
    "        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 오차:  0.34 | 테스트 정확도: 85.00\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋으로 모델 성능 시험\n",
    "model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iter)\n",
    "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq 기계 번역\n",
    "- 2010년 이후 화제가 된 머신러닝 모델이 있다.\n",
    "- 언어를 다른 언어로 해석해주는 뉴럴 기계 번역(neural machine translation) 모델이다\n",
    "- RNN 기반의 번역 모델인 Sequence to Sequence 모델은 기계 번역의 새로운 패러다임을 열었다고 할 정도로 뛰어난 성능을 보인다\n",
    "- Seq2Seq 모델은 시퀀스를 입력받아 또다른 시퀀스를 출력\n",
    "- 한마디로 다른 문장을 번역해주는 모델\n",
    "- 이러한 능력을 학습하려면 병렬 말뭉치(parallel corpora) 라고 하는 원문과 번역문이 쌍을 이루는 형태의 많은 텍스트 데이터가 필요\n",
    "- 이번 예제는 Seq2Seq 모델을 아주 간소화\n",
    "- Seq2Seq 개요\n",
    "    - 각자 다른 역할을 하는 RNN을 이어붙인 모델\n",
    "    - 외국어를 한국어로 번역하는 과정\n",
    "        - 외국어 문장을 읽고 의미를 이해\n",
    "        - 의미를 생각하면서 한국어 단어를 한자 한자 문맥에 맞게 적어감\n",
    "    - 이 두역할을 인코더와 디코더라는 두 RNN 에 부여함으로써 번역.\n",
    "    <img src=\"./7_7_기계번역모델.png\" width=\"70%\" height=\"70%\">\n",
    "    - 인코더\n",
    "        - 원문의 내용을 학습하는 RNN\n",
    "        - 모든 단어를 입력받아 문장의 뜻을 내포하는 하나의 고정 크기 텐서를 만듬\n",
    "        - 이렇게 압축된 텐서는 원문의 뜻과 내용을 압축하고 있다고 하여 문맥벡터(context vector)라고 함\n",
    "        - 인코더는 원문속의 토큰을 차례로 입력받음\n",
    "        - 원문 마지막 토큰에 해당하는 은닉 벡터는 원문의 뜻을 모두 내포하는 문맥 벡터\n",
    "    - 디코더\n",
    "        - 인코더와 마찬가지로 RNN 모델\n",
    "        - 인코더에게서 원문 문맥 벡터를 이어받아 번역문 속의 토큰을 차례대로 예상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq 모델 구현하기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 언어로 된 문장을 다른 언어로 번역하는 작업을 할 때 보통 단어를 문장의 최소 단위로 임베딩을 한다\n",
    "# 그러나 예제에서는 글자단위 캐릭터로 임베딩\n",
    "\n",
    "# 영문만 다룰 것으로 256개의 글자를 표현\n",
    "vocab_size = 256 # 총 아스키 코드 수\n",
    "# 아스키 코드 배열로 정의 후 파이토치 텐서로 변환\n",
    "x_ = list(map(ord, 'hola')) # ord 함수는 chr 함수와 반대, 캐릭터를 아스키 코드로 변환\n",
    "y_ = list(map(ord, 'hello'))\n",
    "x = torch.LongTensor(x_)\n",
    "y = torch.LongTensor(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.n_layers = 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.encoder = nn.GRU(hidden_size, hidden_size)\n",
    "        self.decoder = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        # 디코더가 번역문의 다음 토큰을 예상해내는 작은 신경망을 하나 더 만들어준다.\n",
    "        self.project = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        initial_state = self._init_state()\n",
    "        embedding = self.embedding(inputs).unsqueeze(1)\n",
    "        \n",
    "        # 원문을 인코더에 입력시켜 문맥 벡터인 encoder_state를 만들어 낸다\n",
    "        # 그리고 이 벡터를 디코더의 첫 번째 은닉 벡터 decoder_state로 지정\n",
    "        # 디코더가 번역문의 첫 번째 토큰을 예상하려면 인코더의 문맥 벡터와 문장 시작 토큰을 입력 데이터로 받아야함\n",
    "        encoder_output, encoder_state = self.encoder(embedding, initial_state)\n",
    "        decoder_state = encoder_state\n",
    "        decoder_input = torch.LongTensor([0]) # 디코더의 문장 시작을 알림\n",
    "        \n",
    "        # 디코더는 문장 시작 토큰인 아스키번호 0 을 이용해 번역문 'hola' 의 'h' 토큰을 예측\n",
    "        # 다음 반복에선 'h' 토큰을 이용해 'o' 토큰을 예측\n",
    "        # 반복문으로 구현하여 번역 토큰을 순서대로 예상하고 저장\n",
    "        outputs = []\n",
    "        for i in range(targets.size()[0]):\n",
    "            # 디코더는 첫 번째 토큰과 인코더의 문맥 벡터를 동시에 입력 받음\n",
    "            decoder_input = self.embedding(decoder_input).unsqueeze(1)\n",
    "            decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)\n",
    "            \n",
    "            # 디코더 결과 값은 다시 디코더 모델에 입력\n",
    "            # 디코더 출력 값이 신경망의 마지막 층인 Softmax 층을 거치면 번역문의 다음 예상 글자가 나옴\n",
    "            # 이 예상 결과를 outputs 텐서에 저장해 오차를 계산할 때 사용\n",
    "            # 디코더의 출력값으로 다음 글자 예측\n",
    "            projection = self.project(decoder_output)\n",
    "            outputs.append(projection)\n",
    "            \n",
    "            # 풍부한 데이터를 바탕으로 학습하는 모델이면 디코더가 예측한 토큰을 \n",
    "            # 다음 반복에서 입력될 토큰으로 갱신해 주는것이 정석\n",
    "            # 학습이 부족하면 잘못된 예측 토큰이 입력으로 사용될 가능성이 높다\n",
    "            # 이를 방지하는 방법 중 티처 포싱(teacher forcing)이 가장 대표적\n",
    "            # 실제 번역문의 토큰을 디코더의 전 출력값 대신 입력으로 사용해 학습을 가속\n",
    "            \n",
    "            # 디처 포싱을 이용한 디코더 입력 갱신\n",
    "            decoder_input = torch.LongTensor([targets[i]])\n",
    "            \n",
    "        outputs = torch.stack(outputs).squeeze()\n",
    "        return outputs\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        # parameters() 함수는 신경망 모듈 의 가중치 정보들을 반복자 형태로 반환\n",
    "        # next(self.parameters()).data는 nn.GRU 모듈의 첫번째 가중치 텐서를 추출\n",
    "        # 그후 new() 함수를 호출해 모델의 가중치와 같은 모양인 (n_layers, batch_size, hidden_dim) 모양을 \n",
    "        # 갖춘 텐서로 변환후 zero_() 함수를 호출해 텐서 내 모든 값을 0으로 초기화\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_size).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 반복:0 오차: 5.4319329261779785\n",
      "['y', '+', '½', '\\x95', '\\x95']\n",
      "\n",
      " 반복:100 오차: 1.9381275177001953\n",
      "['l', 'l', 'l', 'l', 'l']\n",
      "\n",
      " 반복:200 오차: 1.0298659801483154\n",
      "['h', 'l', 'l', 'l', 'l']\n",
      "\n",
      " 반복:300 오차: 0.697368323802948\n",
      "['h', 'e', 'l', 'l', 'o']\n",
      "\n",
      " 반복:400 오차: 0.4219569265842438\n",
      "['h', 'e', 'l', 'l', 'o']\n",
      "\n",
      " 반복:500 오차: 0.23388533294200897\n",
      "['h', 'e', 'l', 'l', 'o']\n",
      "\n",
      " 반복:600 오차: 0.1421622931957245\n",
      "['h', 'e', 'l', 'l', 'o']\n",
      "\n",
      " 반복:700 오차: 0.09422233700752258\n",
      "['h', 'e', 'l', 'l', 'o']\n",
      "\n",
      " 반복:800 오차: 0.06706565618515015\n",
      "['h', 'e', 'l', 'l', 'o']\n",
      "\n",
      " 반복:900 오차: 0.049924395978450775\n",
      "['h', 'e', 'l', 'l', 'o']\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작 전 교차엔트로피 오차를 구하는 CrossEntropyLoss 클래스와 최적화 알고리즘 정의\n",
    "seq2seq = Seq2Seq(vocab_size, 16)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=1e-3)\n",
    "\n",
    "log = []\n",
    "for i in range(1000):\n",
    "    prediction = seq2seq(x, y)\n",
    "    loss = criterion(prediction, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_val = loss.data\n",
    "    log.append(loss_val)\n",
    "    if i % 100 == 0:\n",
    "        print('\\n 반복:%d 오차: %s' %(i, loss_val.item()))\n",
    "        _, top1 = prediction.data.topk(1, 1)\n",
    "        print([chr(c) for c in top1.squeeze().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
